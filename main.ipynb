{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Time Series is Worth 64 Words: Long-term Forecasting with Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import PatchTSTForPrediction, PatchTSTConfig\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.make_dataset import get_train_test_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_PATH = os.getcwd()\n",
    "\n",
    "data_path = os.path.join(DIR_PATH, 'dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture des données et création des datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 512\n",
    "sliding_window = 16\n",
    "forcasting_horizon = 96\n",
    "batch_size = 64\n",
    "\n",
    "train_set, test_set = get_train_test_datasets(\n",
    "    data_path+'/ETTm1.csv', \n",
    "    test_size=0.2, \n",
    "    window_size=window_size,\n",
    "    sliding_size=sliding_window,\n",
    "    forcasting_horizon=forcasting_horizon, \n",
    "    kind='sliding_window'\n",
    "    )\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=10, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=10, shuffle=True)\n",
    "\n",
    "num_channels = next(iter(train_loader))[0].shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PatchTSTConfig {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"activation_function\": \"gelu\",\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bias\": true,\n",
       "  \"channel_attention\": false,\n",
       "  \"channel_consistent_masking\": false,\n",
       "  \"context_length\": 512,\n",
       "  \"d_model\": 128,\n",
       "  \"distribution_output\": \"student_t\",\n",
       "  \"do_mask_input\": null,\n",
       "  \"dropout\": 0.2,\n",
       "  \"ff_dropout\": 0.0,\n",
       "  \"ffn_dim\": 256,\n",
       "  \"head_dropout\": 0.2,\n",
       "  \"init_std\": 0.02,\n",
       "  \"loss\": \"mse\",\n",
       "  \"mask_type\": \"random\",\n",
       "  \"mask_value\": 0,\n",
       "  \"model_type\": \"patchtst\",\n",
       "  \"norm_eps\": 1e-05,\n",
       "  \"norm_type\": \"batchnorm\",\n",
       "  \"num_attention_heads\": 16,\n",
       "  \"num_forecast_mask_patches\": [\n",
       "    2\n",
       "  ],\n",
       "  \"num_hidden_layers\": 3,\n",
       "  \"num_input_channels\": 7,\n",
       "  \"num_parallel_samples\": 100,\n",
       "  \"num_targets\": 1,\n",
       "  \"output_range\": null,\n",
       "  \"patch_length\": 1,\n",
       "  \"patch_stride\": 16,\n",
       "  \"path_dropout\": 0.0,\n",
       "  \"pooling_type\": null,\n",
       "  \"positional_dropout\": 0.0,\n",
       "  \"positional_encoding_type\": \"sincos\",\n",
       "  \"pre_norm\": true,\n",
       "  \"prediction_length\": 96,\n",
       "  \"random_mask_ratio\": 0.4,\n",
       "  \"scaling\": \"std\",\n",
       "  \"share_embedding\": true,\n",
       "  \"share_projection\": true,\n",
       "  \"transformers_version\": \"4.47.1\",\n",
       "  \"unmasked_channel_indices\": null,\n",
       "  \"use_cls_token\": false\n",
       "}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializing an PatchTST configuration with 12 time steps for prediction\n",
    "configuration = PatchTSTConfig(\n",
    "    num_input_channels=num_channels,\n",
    "    context_length=window_size,\n",
    "    patch_stride=sliding_window,\n",
    "    prediction_length=forcasting_horizon,\n",
    "    random_mask_ratio=0.4,\n",
    "    d_model=128,\n",
    "    num_attention_heads=16,\n",
    "    num_hidden_layers=3,\n",
    "    ffn_dim=256,\n",
    "    dropout=0.2,\n",
    "    head_dropout=0.2,\n",
    "    pooling_type=None,\n",
    "    channel_attention=False,\n",
    "    scaling=\"std\",\n",
    "    loss=\"mse\",\n",
    "    pre_norm=True,\n",
    "    norm_type=\"batchnorm\"\n",
    ")\n",
    "\n",
    "# Randomly initializing a model (with random weights) from the configuration\n",
    "model = PatchTSTForPrediction(configuration)\n",
    "\n",
    "# Accessing the model configuration\n",
    "configuration = model.config\n",
    "\n",
    "configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, optimizer, criterion, batch_size, num_epochs, train_set, device):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0\n",
    "        num_samples = 0\n",
    "        dloader_train = tqdm(train_loader, unit=\"batches\")\n",
    "        for i, (x, y) in enumerate(dloader_train, 1):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = model(x.permute(0,2,1))[\"prediction_outputs\"]\n",
    "            loss = criterion(y_hat, y.permute(0,2,1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * len(x)\n",
    "            num_samples += len(x)\n",
    "\n",
    "            desc = \"TRAIN : Epoch [{}/{}] - loss : {:.3f} \".format(epoch+1, num_epochs, train_loss/num_samples/y_hat.shape[2])\n",
    "            dloader_train.set_description(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAIN : Epoch [1/10] - loss : 0.618 : 100%|██████████| 54/54 [00:04<00:00, 12.03batches/s]\n",
      "TRAIN : Epoch [2/10] - loss : 0.608 : 100%|██████████| 54/54 [00:04<00:00, 13.30batches/s]\n",
      "TRAIN : Epoch [3/10] - loss : 0.602 : 100%|██████████| 54/54 [00:04<00:00, 13.30batches/s]\n",
      "TRAIN : Epoch [4/10] - loss : 0.597 : 100%|██████████| 54/54 [00:04<00:00, 13.31batches/s]\n",
      "TRAIN : Epoch [5/10] - loss : 0.589 : 100%|██████████| 54/54 [00:04<00:00, 13.29batches/s]\n",
      "TRAIN : Epoch [6/10] - loss : 0.586 : 100%|██████████| 54/54 [00:04<00:00, 13.22batches/s]\n",
      "TRAIN : Epoch [7/10] - loss : 0.576 : 100%|██████████| 54/54 [00:04<00:00, 13.33batches/s]\n",
      "TRAIN : Epoch [8/10] - loss : 0.574 : 100%|██████████| 54/54 [00:04<00:00, 13.34batches/s]\n",
      "TRAIN : Epoch [9/10] - loss : 0.573 : 100%|██████████| 54/54 [00:04<00:00, 13.32batches/s]\n",
      "TRAIN : Epoch [10/10] - loss : 0.560 : 100%|██████████| 54/54 [00:04<00:00, 13.34batches/s]\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, criterion, batch_size, num_epochs, train_set, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
